{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "### Megan Robertson and Anna Yanchenko\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Abstract\n",
    "\n",
    "Latent Dirichlet Analysis (LDA) is a method used to model the generative process of creating discrete data such as text corpora. LDA can take a large amount of data and create descriptions of that data. This method reduces the size of the data to these short descriptions while still maintaining the relationships necessary to carry out various inferences.  For this project, we implemented the LDA algorithm presented in the paper *Latent Dirichlet Allocation*, written by David M. Blei, Andrew Y. Ng, and Michael I. Jordan. The implementation of the algorithm is used to analyze the text of some famous American historical documents and to create clusters of movies based on user rating data. \n",
    "\n",
    "## II. Background\n",
    "\n",
    "LDA is most well-known for its application in the analysis of text data. It is used to create topics for documents, classify documents based on these topics and to determine which documents are similar to one another. LDA is based on the \"bag-of-words\" assumption, that is, that there is exchangeability between documents and words in the corpus. The algorithm can also be used in other problems that have a similar structure to the document generating method described above. For example, in Blei et.al, 2003, the authors used a data set where web site users provide ratings about movies they enjoy. In this example, the users are analogous to the documents and their preferred movies are “words”.  The model is used to predict what movies a user may like based on their previous ratings.\n",
    "\n",
    "The algorithm assumes a generative process for the creation of each document **w** in a corpus D.  For each document in a corpus, the following generative process is assumed  (Blei et al., pg. 996):\n",
    "\n",
    "1. Select number of words for the document, N $\\sim Poission(\\xi$)\n",
    "2. Choose $\\theta \\sim Dirichilet(\\alpha)$\n",
    "3. For each word, $w_n$:\n",
    "  \n",
    " a. Select topic, $z_n \\sim Multinomial(\\theta)$\n",
    "\n",
    " b. Select a word $w_n$ from $p(w_n |z_n, \\beta)$\n",
    "\n",
    "This process leads to the following posterior distribution:\n",
    "\n",
    "$$ p(\\theta, \\textbf{z} | \\textbf{w}, \\alpha, \\beta) = \\dfrac{p(\\theta, \\textbf{z}, \\textbf{w} | \\alpha, \\beta)}{p(\\textbf{w}, \\alpha, \\beta)}$$\n",
    "\n",
    "However, this posterior is intractable, and must be approximated. \n",
    "\n",
    "There are multiple alternative algorithms that can be used for text analysis. These include the term-frequency inverse-document-frequency (tf-idf) matrix, latent semantic indexing (LSI) and the probabilistic latent semantic indexing (pLSI) model. In Section 7 of the paper, the authors present the results of document modeling and document classification. In this section, the authors’ results show that their implementation of LDA performs better than competing methods in terms of perplexity measure, where better generalization performance is defined by a smaller perplexity measure. \n",
    "\n",
    "This section demonstrates that other methods are prone to overfitting. As the number of topic increases, some of the alternative algorithms induce words that have small probabilities. This occurs because the documents in the corpus are divided into more collections. This can result in the perplexity measure becoming very large for these alternative algorithms. The problem comes from the requirement that the topic proportions in a future document must be seen in at least one of the training documents. On the other hand, LDA does not have this overfitting problem. See Section 7 of Blei et al for more details.\n",
    "\n",
    "A disadvantage of LDA is that exact inference of the posterior is impossible as a result of intractability. Thus, it is necessary for the user to implement an approximating technique such as variational inference, a Gibbs sampler, or another technique. The approximation of the posterior can be computationally intensive and time-consuming.  Additionally, for LDA, the number of topics $k$ must be decided before the algorithm is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the background, the posterior is intractable. Therefore it is necessary to approximate the posterior, and this is done using variational inference. This approximation is:\n",
    "\n",
    "$$q(\\theta, \\textbf{z} | \\gamma, \\phi) = q(\\theta | \\phi) \\prod_{n=1}^N q(z_n | \\phi_n)$$\n",
    "\n",
    "where $\\gamma$ is a Dirichlet random variable and ($\\phi_1, ..., \\phi_N)$ are Multinomial.\n",
    "\n",
    "These variables are used to approximate the posterior using \"an alternating variational EM procedure that maximizes a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$, and then, for fixed values of the variational parameters, maximizes the lower bound with respect to the model parameters $\\alpha$ and $\\beta$\" (Blei et al, 1005).\n",
    "\n",
    "The details of the derivations and equations can be found in the Appendix of the paper (Blei et al, 1018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Code\n",
    "\n",
    "The LDA algorithm was implemented using the Python programming language in the Jupyter notebook environment. Descriptions for the functions used in the algorithm are included with the code below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\tTesting and Base Cases\n",
    "\n",
    "In addition to the implementation described above, various checks have been included in the function to prevent incorrect arguments.  One check is that the number of topics specified by the user must be greater than one. It is not possible to have zero or less than zero topics, and one topic would just be described by the entire document. In addition, there is a check in place to ensure that the corpus is not empty. Lastly, there are checks to ensure that the tolerance is greater than zero and that the entry for needToSplit is either zero or one. These checks print messages that inform the user of why their input was problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code included below is the optimized code for the project. For the unoptimized versions referenced later in the paper, see the Appendix, section A.1, also included in the Submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): stop-words in /Users/annayanchenko/anaconda3/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): nltk in /Users/annayanchenko/anaconda3/lib/python3.5/site-packages\n"
     ]
    }
   ],
   "source": [
    "! pip install stop_words\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toy Example Corpus **\n",
    "From HW5 Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "s1 = \"The quick brown fox\"\n",
    "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "s3 = \"The the the lazy dog elephant.\"\n",
    "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "docs = collections.OrderedDict()\n",
    "docs[\"s1\"] = s1\n",
    "docs[\"s2\"] = s2\n",
    "docs[\"s3\"] = s3\n",
    "docs[\"s4\"] = s4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to make Corpus Matrix** \n",
    "\n",
    "\n",
    "**make_word_matrix(corpus, needToSplit)**\n",
    "\n",
    "Inputs: \n",
    "- *corpus*: a dictionary of text documents, each element of the dictionary is a separate document in string format\n",
    "- *needToSplit*: if each document is one single string and not split into individual words, needToSplit = 1, otherwise,\n",
    "    if the document is already split into individual words, needToSplit = 0\n",
    "    \n",
    "Output: list[c, wordOrder, M]\n",
    "- *c*: a list of corpus matricies, one matrix for each document. Each row corresponds to each word in that document, while each colulmn corresponds to a unique word in the vocabulary (V possible words) for all documents.  Following the notation of Blei, et. al., the nth word in the vocabulary is represented by $w^n = 1$ and $w^m$ = 0 for all $m\\neq n$\n",
    "- *wordOrder*: a list of the unique words in the vocabulary for the corpus; these are the column labels for each matrix in c\n",
    "- *M*: the number of documents in the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import stop_words\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "\n",
    "#Function to make document, word matricies for LDA#\n",
    "def make_word_matrix(corpus, needToSplit):\n",
    "    \n",
    "    #define dictionary to store \"cleaned\" words\n",
    "    cleanCorpus = {}\n",
    "    #Define stop words\n",
    "    stopWords = get_stop_words('english')\n",
    "    \n",
    "    \n",
    "    #Initialize stemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    \n",
    "    #Define list to store corpus data#\n",
    "    c = []\n",
    "    #Define list to store order of words for each document#\n",
    "    wordOrder = []\n",
    "    #Define table to remove punctuation\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "    M = len(corpus)\n",
    "    \n",
    "    #Check to make sure that dictionary isn't empty#\n",
    "    if M ==0:\n",
    "        print(\"Input dictionary is empty\")\n",
    "        return;\n",
    "    \n",
    "    removePunc = string.punctuation\n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "        \n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "\n",
    "        #Remove stop words#\n",
    "        text = [word for word in words if word not in stopWords]\n",
    "        # stem tokens\n",
    "        text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "        cleanCorpus[i] = text\n",
    "        #Find number of unique words in each document#\n",
    "        Vwords = list(set(text))\n",
    "        wordOrder.append(Vwords)\n",
    "\n",
    "    #Find unique words in the corpus, this is the vocabulary#    \n",
    "    wordOrder = list(set(x for l in wordOrder for x in l))\n",
    "    wordOrder = sorted(wordOrder)\n",
    "    #Find the number of unique words in the corpus vocabulary#\n",
    "    V = len(wordOrder)\n",
    "    \n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in cleanCorpus:\n",
    "        text = cleanCorpus[i]\n",
    "        N = len(text)\n",
    "        #Create matrix to store words for each document#\n",
    "        wordsMat = np.zeros((N, V))\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            #Find which word in vocabulary current word in document corresponds to#\n",
    "            v = wordOrder.index(word)\n",
    "            #Add a 1 to that column in the wordsMat matrix#\n",
    "            wordsMat[count, v] = 1\n",
    "            count += 1\n",
    "        c.append(wordsMat)\n",
    "\n",
    "    return [c, wordOrder, M] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output for toy docs example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "         [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])],\n",
       " ['brown',\n",
       "  'dog',\n",
       "  'eleph',\n",
       "  'fox',\n",
       "  'jump',\n",
       "  'lazi',\n",
       "  'lion',\n",
       "  'peacock',\n",
       "  'quick',\n",
       "  'tiger'],\n",
       " 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix = make_word_matrix(docs, 1)\n",
    "corpusMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E-Step:** This function uses variational inference to perform the E step in the EM algorithm to estimate the paramteters in the model.  See page 1004 of Blei, et. al. for the derivation.\n",
    "\n",
    "**Estep(k, d, alpha, beta, corpusMatrix, tol)**\n",
    "\n",
    "**Inputs:**\n",
    "- *k*: the number of topics assumed for the LDA model, an integer\n",
    "- *d*: the current document in the corpus to perform the E step on, an integer\n",
    "- *alpha*: the current alpha vector for the model, a k-dimensional vector\n",
    "- *beta*: the current beta matrix for the model, a k x V matrix\n",
    "- *corpusMatrix*: the output of make_word_matrix() from above\n",
    "- *tol*: the convergence criteria for the values of phi and gamma found in the Estep\n",
    "\n",
    "**Output: list(newPhi, gamma)**\n",
    "- *newPhi*: the updated result for phi from the Estep, a N x k matrix (N is the number of words in document d)\n",
    "- *gamma*: the updated result for gamma from the Estep, a k-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Estep(k, d, alpha, beta, corpusMatrix, tol):    \n",
    "    \n",
    "    #storing the total number of words and the number of unique words\n",
    "    N = corpusMatrix[0][d].shape[0]\n",
    "    V = corpusMatrix[0][d].shape[1]\n",
    "    \n",
    "    #initialize phi and gamma\n",
    "    oldPhi  = np.full(shape = (N,k), fill_value = 1/k)\n",
    "    gamma = alpha + N/k\n",
    "    newPhi = oldPhi\n",
    "    converge = 0 \n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while converge == 0:\n",
    "        newPhi  = np.zeros(shape = (N,k))\n",
    "        #Update phi\n",
    "        for n in range(0, N):\n",
    "            for i in range(0,k):\n",
    "                newPhi[n,i] = (beta[i, list(corpusMatrix[0][d][n,:]).index(1)])*np.exp(scipy.special.psi(gamma[i]))\n",
    "        newPhi = newPhi/np.sum(newPhi, axis = 1)[:, np.newaxis] #normalizing the rows of new phi\n",
    "\n",
    "        for i in range(0,k):\n",
    "            gamma[i] = alpha[i] + np.sum(newPhi[:, i]) #updating gamma\n",
    "\n",
    "        #Check convergence criteria\n",
    "        criteria = (1/(N*k)*np.sum((newPhi - oldPhi)**2))**0.5\n",
    "        if criteria < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            oldPhi = newPhi\n",
    "            count = count +1\n",
    "            converge = 0\n",
    "    return (newPhi, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Estimation\n",
    "\n",
    "**M Step:** In the E step above, we maximized a lower bound with respect to gamma and phi, and in the M step, for fixed values of these variational parameters, we maximize the lower bound of the log likelihood with repsect to alpha and beta to update these values (combined, these two steps give approximate empirical Bayes estimates for the LDA model).  See pg. 1006 and appendix A.2 in Blei et. al. for the derivation.  \n",
    "\n",
    "The cy_alphaUpdate() function uses the linear Newton-Rhapson method to update the Dirichlet parameters, alpha, while the cy_betaUpdate() function maximizes for beta.\n",
    "\n",
    "**cy_alphaUpdate(k, M, alphaOld, gamma, tol):**\n",
    "\n",
    "**Inputs:**\n",
    "- *k*: the number of topics, an integer\n",
    "- *M*: the number of documents in the corpus, an integer\n",
    "- *alphaOld*: the previous alpha value, a k-dimensional vector\n",
    "- *gamma*: the value which maximized the lower bound in the Estep above, a k-dimensional vector\n",
    "- *tol*:  the convergence criteria for the linear Newton-Rhapson method to update alpha\n",
    "\n",
    "**Output**: alpaNew\n",
    "- *alphaNew*: the updated version of alpha which maximizes the lower bound of the log likelihood, a k-dimensional vector\n",
    "\n",
    "\n",
    "\n",
    "**cy_betaUpdate(k, M, phi_dense, gamma, c_1, c_2):**\n",
    "\n",
    "**Inputs:**\n",
    "- *k*: the number of topics, an integer\n",
    "- *M*: the number of documents in the corpus, an integer\n",
    "- *phi_dense*: a list of dense matricies that are the output from the Estep above run on each document, each matrix is max $N_i$ x k, where max $N_i$ is the maximum number of words in a single document in the corpus\n",
    "- *c_1*: the number of words in each document in the corpus, a M-dimensional vector\n",
    "- *c_2*: a list of dense matricies, the same as the c portion of the output of make_word_matrix above, but each matrix is now the same dimensions (with rows of zeros added as necessary), each matrix is max $N_i$ x V, where max $N_i$ is the maximum number of words in a single document in the corpus\n",
    "\n",
    "**Outputs**: beta\n",
    "- *beta*: the updated version of beta which maximizes the lower bound of the log likelihood, a k x V matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython  -lgsl\n",
    "\n",
    "import cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import scipy\n",
    "from cython_gsl cimport *\n",
    "\n",
    "#Update alpha using linear Newton-Rhapson Method#\n",
    "\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "cpdef double[:] cy_alphaUpdate(int k, int M, double[:] alphaOld, double[:, :] gamma, double tol):\n",
    "    cdef double[:] h = np.zeros(k)\n",
    "    cdef double[:] g = np.zeros(k)\n",
    "    cdef double[:] alphaNew = np.zeros(k)\n",
    "\n",
    "    cdef int converge\n",
    "    cdef int i, d\n",
    "    cdef double docSum\n",
    "    cdef double[:] step = np.zeros(k)\n",
    "    cdef double c, s1, s2\n",
    "\n",
    "    cdef double alpha_sum = 0\n",
    "    for i in range(k):\n",
    "        alpha_sum += alphaOld[i]\n",
    "\n",
    "    converge = 0\n",
    "    while converge == 0:\n",
    "        z = -gsl_sf_psi_n(1, alpha_sum)\n",
    "\n",
    "        s1 = 0\n",
    "        s2 = 1.0/z\n",
    "        for i in range(0, k):\n",
    "            docSum = 0\n",
    "            for d in range(M):\n",
    "                docSum += gsl_sf_psi(gamma[d,i]) - gsl_sf_psi(sum(gamma[d]))\n",
    "            g[i] = M*(gsl_sf_psi(alpha_sum) - gsl_sf_psi(alphaOld[i])) + docSum\n",
    "            h[i] = M*gsl_sf_psi_n(1, alphaOld[i])\n",
    "            \n",
    "            s1 += g[i]/h[i]\n",
    "            s2 += 1.0/h[i]\n",
    "        c = s1/s2\n",
    "\n",
    "        for i in range(k):\n",
    "            step[i] = (g[i] - c)/h[i]\n",
    "        # step = (g - c)/h\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            for i in range(k):\n",
    "                alphaNew[i] = alphaOld[i] + step[i]\n",
    "            alphaOld = alphaNew\n",
    "\n",
    "    return alphaNew\n",
    "\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "cpdef double[:, :] cy_betaUpdate(int k, int M, double[:, :, :] phi_dense,long[:] c_1, double[:, :, :] c_2):\n",
    "\n",
    "    cdef int i, j, d, n\n",
    "    cdef int Nd\n",
    "    cdef double wordSum\n",
    "\n",
    "    #Calculate beta#\n",
    "    cdef int V = c_2[0].shape[1]\n",
    "    cdef double[:, :] beta = np.zeros(shape = (k,V))\n",
    "\n",
    "    for i in range(0,k):\n",
    "        for j in range(0,V):\n",
    "            wordSum = 0\n",
    "            for d in range(M):\n",
    "                Nd = c_1[d] # c[d].shape[0]\n",
    "                for n in range(0, Nd):\n",
    "                    wordSum += phi_dense[d, n,i]*c_2[d, n,j]\n",
    "            beta[i,j] = wordSum\n",
    "    #Normalize the rows of beta#\n",
    "    beta = beta/np.sum(beta, axis = 1)[:, np.newaxis]\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Function: LDA(k, corpusMatrix, tol)\n",
    "\n",
    "For each document d, the function runs the E step, then for all documents runs the M step update for alpha and beta.  This variational update and parameter estimation update continues until alpha or beta converges to within the specified tolerance.  The final values of phi, gamma, alpha and beta are returned for all M documents in a list.\n",
    "\n",
    "**Inputs:**\n",
    "- *k*: the number of topics, an integer\n",
    "- *corpusMatrix*: the output of make_word_matrix() above for the desired corpus\n",
    "- *tol*: the convergence criteria to be used in the E-step, alpha update in the M-step and the convergence to be used for the overall maximization \n",
    "\n",
    "**Output:** list([phi, gamma, alphaNew, betaNew])\n",
    "- *phi*: a list of matricies, the final phi matrix values for each document in the corpus\n",
    "- *gamma*: a list of vectors, the final gamma vector for each document in the corpus\n",
    "- *alphaNew*: the final alpha vector, a k-dimensional vector\n",
    "- *betaNew*: the final beta matrix, a k x V matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDA(k, corpusMatrix, tol):\n",
    "\n",
    "    # create rectangular matrices for c\n",
    "    c = corpusMatrix[0]\n",
    "    r_c = len(c)\n",
    "    c_1 = np.array([c_.shape[0] for c_ in c]).astype('int')\n",
    "    n_c = max(c_1)\n",
    "    p_c = c[0].shape[1]\n",
    "    c_2 = np.zeros((r_c, n_c, p_c))\n",
    "    for i, j in enumerate(c_1):\n",
    "        c_2[i, :j, :] = c[i]\n",
    "    \n",
    "    ##Check for proper input##\n",
    "    if isinstance(k, int) != True or k <= 0:\n",
    "        print(\"Number of topics must be a positive integer\")\n",
    "        return;\n",
    "    \n",
    "    if tol <=0:\n",
    "        print(\"Convergence tolerance must be positive\")\n",
    "        return;\n",
    "    \n",
    "    M = corpusMatrix[2]\n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    alphaOld = np.full(shape = k, fill_value = 50/k) + np.random.rand(k)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(k, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    while converge == 0:\n",
    "        phi = []\n",
    "        gamma = []\n",
    "        \n",
    "        ##E-step##\n",
    "        #looping through the number of documents\n",
    "        for d in range(0,M): #M is the number of documents\n",
    "            phiT, gammaT = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "            phi.append(phiT)\n",
    "            gamma.append(gammaT)\n",
    "            \n",
    "        # create rectangular matrices for phi\n",
    "        r = len(phi)\n",
    "        phi_1 = np.array([p_.shape[0] for p_ in phi]).astype('int')\n",
    "        n = max(phi_1)\n",
    "        p = phi[0].shape[1]\n",
    "        phi_2 = np.zeros((r, n, p))\n",
    "        for i, j in enumerate(phi_1):\n",
    "            phi_2[i, :j, :] = phi[i]\n",
    "\n",
    "        ##M - step##\n",
    "        #Update alpha and beta#     \n",
    "        gamma = np.array(gamma)\n",
    "        alphaNew = np.array(cy_alphaUpdate(k, M, alphaOld, gamma, tol))\n",
    "        betaNew = np.array(cy_betaUpdate(k, M, phi_2, c_1, c_2))\n",
    "    \n",
    "        if np.linalg.norm(alphaOld - alphaNew) < tol or np.linalg.norm(betaOld - betaNew) < tol:\n",
    "       \n",
    "            converge =1\n",
    "        else: \n",
    "            converge =0\n",
    "            alphaOld = alphaNew\n",
    "            betaOld = betaNew\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time for toy example to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.76 ms, sys: 623 µs, total: 5.38 ms\n",
      "Wall time: 5.85 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(97)\n",
    "res = LDA(3, corpusMatrix, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toy example LDA output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.09689023,  0.5125238 ,  0.39058597],\n",
       "        [ 0.10309406,  0.63093592,  0.26597002],\n",
       "        [ 0.5518897 ,  0.17445205,  0.27365826]]),\n",
       " array([[ 0.3948353 ,  0.14007042,  0.46509427],\n",
       "        [ 0.1557428 ,  0.20170723,  0.64254998],\n",
       "        [ 0.24561568,  0.43189135,  0.32249297],\n",
       "        [ 0.17846235,  0.63008699,  0.19145066],\n",
       "        [ 0.17572041,  0.23097513,  0.59330447]]),\n",
       " array([[ 0.16226174,  0.6583813 ,  0.17935696],\n",
       "        [ 0.40071343,  0.14074978,  0.45853679],\n",
       "        [ 0.17916578,  0.23317472,  0.5876595 ]]),\n",
       " array([[ 0.11610625,  0.62060252,  0.26329123],\n",
       "        [ 0.58413674,  0.16126668,  0.25459658],\n",
       "        [ 0.72925797,  0.13686583,  0.1338762 ],\n",
       "        [ 0.72925797,  0.13686583,  0.1338762 ],\n",
       "        [ 0.72925797,  0.13686583,  0.1338762 ],\n",
       "        [ 0.72925797,  0.13686583,  0.1338762 ]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_toy = res[0][0]\n",
    "phi_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.63161569,  18.71214015,  17.96387036],\n",
       "       [ 19.03011823,  19.0289595 ,  19.24854846],\n",
       "       [ 18.62188265,  18.42653418,  18.25920936],\n",
       "       [ 21.49701656,  18.72356092,  18.08704871]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_toy = res[0][1]\n",
    "gamma_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17.98476059,  17.34928734,  17.03592818])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_toy = res[0][2]\n",
    "alpha_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0350067 ,  0.12705064,  0.056676  ,  0.18142558,  0.46585553,\n",
       "         0.02591351,  0.03922529,  0.02487242,  0.01547355,  0.02850078],\n",
       "       [ 0.23550474,  0.05284256,  0.0873401 ,  0.06317293,  0.10301738,\n",
       "         0.12388906,  0.08126995,  0.03795569,  0.09644273,  0.11856486],\n",
       "       [ 0.09757672,  0.17028432,  0.21772724,  0.09739117,  0.0987278 ,\n",
       "         0.03306697,  0.05945609,  0.11846308,  0.07200999,  0.03529661]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_toy = res[0][3]\n",
    "beta_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to Find Highest Probability Words for Each Topic: **\n",
    "Run on toy example in (d) ii. below\n",
    "\n",
    "**mostCommon(beta, wordList, p):**\n",
    "\n",
    "**Inputs:**\n",
    "- *beta*: the final value for the beta matrix from the LDA function above\n",
    "- *wordList*: the list of the unique words in the vocabulary in the corpus that LDA was run on to find beta, the output of the make_word_matrix()[1]\n",
    "- *p*: the number of words to return for each topic\n",
    "\n",
    "**Output: list(topicWords)**\n",
    "- *topicWords*: for each topic, the words in the corpus that have the highest probability of occuring under  that topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to return the most probable words\n",
    "#p is the number of words you want returned for each topic\n",
    "def mostCommon(beta, wordList, p):\n",
    "    k = beta.shape[0]\n",
    "    topicWords = []\n",
    "    betaDF = pd.DataFrame(beta)\n",
    "    betaDF.columns = wordList\n",
    "    for i in range(0, k):\n",
    "        document = betaDF.loc[i,:]\n",
    "        document.sort(1, ascending = 0)\n",
    "        mostCommon = pd.DataFrame(document[0:p])\n",
    "        topicWords.append(mostCommon)\n",
    "    return(topicWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\tTests for Correctness\n",
    "\n",
    "### *i.\tGenerative Model*\n",
    "\n",
    "A generative model check was utilized in order to evaluate the functionality of the algorithm. A random number was generated in order to determine the number of documents from a Poisson(5) distribution, and the resulting number was six. Then, for each of the six documents, a random number of words for each document was drawn from a Poisson(5) distribution. These documents had between three and seven words. Random values were used to create a $\\beta$ vector, and the $\\alpha$ vector was set to be 50/k, where k is the number of topics and k = 3. The 50/k value was chosen based on the results shown in Gregor Heinrich’s *Parameter Estimation for Text Analysis*  (Heinrich, 2008, 24). This value was chosen such that $\\alpha$ converges logically. With a random initialization, the algorithm returned unrealistic values for $\\alpha$, and it would require much more time to converge.\n",
    "\n",
    "The corpus was defined to have five unique words that are not stop words.\n",
    "The set-up described in the previous paragraph was used to determine which of the words were in each of the documents. This process followed the generative model described in Blei et. al’s paper and above (Blei et al, 2003, 996). Once the documents were generated, our LDA implementation was used on this corpus to estimate the $\\alpha$ and $\\beta$ values. The following table displays the mean squared error (MSE) between the actual $\\alpha$ and $\\beta$ values used to create the corpus and the ones determined by our algorithm.\n",
    "\n",
    "| Alpha MSE | Beta MSE |\n",
    "|-----------|----------|\n",
    "|      1.50 |     0.77 |\n",
    "\n",
    "Thus, the values returned by the algorithm are relatively close to the ones used to generate the corpus. It is not surprising that the MSE for the $\\beta$ vector is larger because there are more elements in the beta vector. The $\\alpha$ vector converges faster than the $\\beta$ vector, so this could be the reason for the larger MSE for $\\beta$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 4.,  1.,  3.,  1.,  0.]),\n",
       " array([ 4.,  3.,  1.]),\n",
       " array([ 3.,  1.,  2.,  1.,  1.,  4.,  0.]),\n",
       " array([ 4.,  4.,  1.,  0.,  1.]),\n",
       " array([ 4.,  2.,  1.,  2.,  3.,  1.,  0.]),\n",
       " array([ 2.,  1.,  1.,  4.,  3.])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate data\n",
    "np.random.seed(7)\n",
    "#Draw number of documents\n",
    "M = np.random.poisson(lam = 5)\n",
    "k = 3  #Number of topics\n",
    "alpha = np.full(shape = k, fill_value = 50/k) +np.random.rand(k)\n",
    "V = 5  #Number of words in vocabulary\n",
    "betaOld = np.random.rand(k, V)\n",
    "betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "corpus = []\n",
    "for i in range(0,M):\n",
    "    #Draw number of words in document i\n",
    "    N = np.random.poisson(5)\n",
    "    theta = np.random.dirichlet(alpha)\n",
    "    docWords = np.zeros(N)\n",
    "    for j in range(0,N):\n",
    "        #Draw latent topics for each word\n",
    "        z = np.random.multinomial(1,theta)\n",
    "        zn = list(z).index(1)\n",
    "        #Draw each word given that latent topic\n",
    "        w = np.random.multinomial(1,betaOld[zn,:])\n",
    "        docWords[j] = list(w).index(1)\n",
    "    corpus.append(docWords)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word choices apple, banana, grape, orange, watermelon\n",
    "s1 = \"Watermelon banana orange banana apple\"\n",
    "s2 = \"Watermelon orange banana\"\n",
    "s3 = \"Orange banana grape banana banana watermelon apple.\"\n",
    "s4 = \"Watermelon watermelon banana apple banana\"\n",
    "s5 = \"Watermelon grape banana grape orange banana apple\"\n",
    "s6 = \"Grape banana banana watermelon orange\"\n",
    "docsTest = collections.OrderedDict()\n",
    "docsTest[\"s1\"] = s1\n",
    "docsTest[\"s2\"] = s2\n",
    "docsTest[\"s3\"] = s3\n",
    "docsTest[\"s4\"] = s4 \n",
    "docsTest[\"s5\"] = s5 \n",
    "docsTest[\"s6\"] = s6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16.7387178 ,  16.93510565,  17.16654917])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30623643,  0.36237236,  0.17175045,  0.02972794,  0.12991282],\n",
       "       [ 0.35935154,  0.08430178,  0.17861983,  0.36788995,  0.00983689],\n",
       "       [ 0.18543475,  0.29337665,  0.07111187,  0.16936021,  0.28071651]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betaOld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17.24430799,  17.92955043,  16.16631559])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatTest = make_word_matrix(docsTest, 1)\n",
    "np.random.seed(1)\n",
    "phi, gamma, alphaNew, betaNew =  LDA(3, corpusMatTest, 0.1)[0]\n",
    "alphaNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11557991,  0.41398094,  0.05114086,  0.11955674,  0.29974155],\n",
       "       [ 0.05963179,  0.6014481 ,  0.09408642,  0.17555618,  0.06927752],\n",
       "       [ 0.21675927,  0.04987748,  0.24766886,  0.17423878,  0.31145561]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betaNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alpha MSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4983354358871279"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(alpha - alphaNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beta MSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76669992887715632"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(betaOld - betaNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ii.\tComparison to Python package*\n",
    "\n",
    "In addition to the generative model, the performance of our LDA implementation was compared to the implementation from the genism package. This implementation can be found at https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html. \n",
    "\n",
    "The results from the Python package and our LDA algorithm were not exactly the same for every topic. For each topic, three of the four words for each topic were the same and the probabilites were similar. However, the fourth word typically differed. This could be a result of some of the differences between the two algorithms. The Python package implmentation requires the user to pass the number of iterations as an argument whereas our implemnentation sets convergence criterion. In addition, the Python implementation randomly initiates all of the parameters. Thus, the differences between the starting positions and the convergence criterion could result in different topics. Therefore, we are satisfied with the similarity found between our LDA implementation and that using the gensim package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"The quick brown fox\"\n",
    "doc_b = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "doc_c =  \"The the the lazy dog elephant.\"\n",
    "doc_d = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, iterations = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Package Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.192*jump + 0.124*brown + 0.106*fox + 0.101*dog'), (1, '0.187*jump + 0.120*fox + 0.117*eleph + 0.115*dog'), (2, '0.176*jump + 0.118*dog + 0.118*eleph + 0.109*fox')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our LDA Function Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annayanchenko/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: FutureWarning: sort is deprecated, use sort_values(inplace=True) for for INPLACE sorting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[              0\n",
       " jump   0.465856\n",
       " fox    0.181426\n",
       " dog    0.127051\n",
       " eleph  0.056676,               1\n",
       " brown  0.235505\n",
       " lazi   0.123889\n",
       " tiger  0.118565\n",
       " jump   0.103017,                 2\n",
       " eleph    0.217727\n",
       " dog      0.170284\n",
       " peacock  0.118463\n",
       " jump     0.098728]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommon(res[0][3], corpusMatrix[1], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.\tProfiling Code and Speed Up\n",
    "\n",
    "We profiled the initial code by implementing the algorithm on a corpus of all of President Clinton’s State of the Union Addresses. The profiling results can be found in the Appendix, Section A.2. It was found that the main bottleneck in the algorithm was the maximization step of the algorithm; about 75% of the time of the LDA algorithm was spent on this step. In the maximization step, the $\\alpha$ update occurs in a while loop, and the $\\beta$ update uses four for loops. The expectation step also slowed down the code, but only accounted for approximately 13% of the time of the LDA implementation. This mostly likely is a result of having fewer for loops than the maximization step. \n",
    "\n",
    "We identified inefficiencies in the matrix construction function and implemented a storage system in order to avoid the unnecessary repetition of some calculations. In addition, during the initial coding of the algorithm, care was taken to avoid for loops and use vectorization where possible. Just in time compilation (Numba) was also added to the code for the algorithm implementation. \n",
    "\n",
    "Given the difficulty of the algorithm and its dependence on previous iterations, it was not clear how parallelization would speed up the implmentation. Thus with the assistance of Professor Chan, the code was adapted so that Cython could be used. It was first necessary to adjust the code to prevent ragged arrays because Cython does not easily work with sparse or ragged arrays. The phi and the corpus matrix both initially used ragged arrays. To fix this, the largest dimension of the matrices was found, and the rest of the matrices were filled in with zeros such that the dimensions matched. In addition, the digamma function from the Cython GSL package replaced that from the scipy package. The final change was using cdef to define the inputs to the functions and the variables used within. \n",
    "\n",
    "\n",
    "\n",
    "**Toy  Example Timing:**\n",
    "\n",
    "| Method| Total Time [s] |\n",
    "|-----------|----------|\n",
    "|      No Speed-Up|    0.00436|\n",
    "|      Numba JIT|    3.02|\n",
    "|     Cython|    0.00416|\n",
    "\n",
    "\n",
    "**Clinton SOTU Timing:**\n",
    "\n",
    "| Method| Total Time  |\n",
    "|-----------|----------|\n",
    "|      No Speed-Up|    4 min, 54 sec|\n",
    "|      Numba JIT|    4 min, 51 sec|\n",
    "|     Cython|    1 min, 30 sec|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As the tables above show, the Cython provided the fastest implementation of the algorithm. For the smaller example, Numba performed the slowest and was only marginally faster in the State of the Union example than no optimization. According to the Numba documentation, there is compilation time associated with Numba. In addition, it would be faster if Numba was used in No Python mode. However, this was not possible given the use of the numpy package. For the State of the Union example, the Cython code provides approximately a three time speed up over the other implementations.  There were 10,640 words used in the State of the Union example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time LDA on 8 Clinton State of the Union Speeches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "#creating a diciontary of the state of the Union Addresses for Clinton\n",
    "fileNames = state_union.fileids()\n",
    "Clinton = fileNames[50:58]\n",
    "\n",
    "ClintonSOTU = {}\n",
    "for i in range(0, len(Clinton)):\n",
    "    ClintonSOTU[Clinton[i].rsplit('.', 1)[0]] = list(nltk.corpus.state_union.words(Clinton[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 1.64 s, total: 1min 40s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(7)\n",
    "ClintCorp = make_word_matrix(ClintonSOTU, 0)\n",
    "Results = LDA(3, ClintCorp, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.\tResults\n",
    "\n",
    "### *i. Topic Analysis*\n",
    "\n",
    "The LDA algorithm was used to determine the topics that occurred in famous documents and speeches from American history. These included the Gettysburg Address, the Declaration of Independence, Martin Luther King’s “I Have A Dream” speech and President John F. Kennedy’s inauguration address. The first implementation defined the number of topics to be three. \n",
    "\n",
    "**American Speech Topics: 3 Topics:**\n",
    "\n",
    "\n",
    "\n",
    "| Topic | Potential Occurring Themes |\n",
    "|-----------|----------|\n",
    "|      0 |     events, causes, opinions, impel |\n",
    "|       1    |     laws, nature, connected, rights, life, happiness    |\n",
    "|        2   |      people, disolve, political, another, God, Creator     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the topic assignments, the following excerpt from The Declaration of Independence is included, where each word is highlighted to correspond to the color of the topic under which it occurs with the highest probability. \n",
    "\n",
    "<span style=\"color:red\">Topic 0</span> \n",
    " <span style=\"color:blue\">Topic 1</span> \n",
    " <span style=\"color:green\">Topic 2</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When in the <span style=\"color:blue\">Course</span>  of <span style=\"color:green\">human</span> <span style=\"color:red\">events</span> , it <span style=\"color:red\">becomes</span> <span style=\"color:green\">necessary</span> for <span style=\"color:green\">one</span> <span style=\"color:green\">people</span> to <span style=\"color:green\">dissolve</span> the <span style=\"color:green\">political</span> <span style=\"color:red\">bands</span> \n",
    "which have <span style=\"color:blue\">connected</span> them with <span style=\"color:green\">another</span>, and to <span style=\"color:blue\">assume</span>, <span style=\"color:blue\">among</span> the <span style=\"color:green\">Powers</span> of the <span style=\"color:green\">earth</span> , the <span style=\"color:green\">separate</span> and <span style=\"color:green\">equal</span> \n",
    "<span style=\"color:blue\">station</span> to which the <span style=\"color:blue\">Laws</span> of <span style=\"color:blue\">Nature</span> and of <span style=\"color:blue\">Nature's</span> <span style=\"color:green\">God</span> <span style=\"color:green\">entitle</span> them, a <span style=\"color:red\">decent</span> <span style=\"color:red\">respect</span> to the <span style=\"color:red\">opinions</span> of \n",
    "<span style=\"color:red\">mankind</span> <span style=\"color:green\">requires</span> that they should \n",
    "<span style=\"color:green\">declare</span> the <span style=\"color:red\">causes</span> which <span style=\"color:red\">impel</span> them to the <span style=\"color:green\">separation</span>.\n",
    "\n",
    "We <span style=\"color:blue\">hold</span> these <span style=\"color:green\">truths</span> to be <span style=\"color:blue\">self-evident</span>, that all <span style=\"color:blue\">men</span> are <span style=\"color:red\">created</span> <span style=\"color:green\">equal</span>, that they are <span style=\"color:blue\">endowed</span> by their \n",
    "<span style=\"color:green\">Creator</span> with <span style=\"color:blue\">certain unalienable Rights</span>, that <span style=\"color:blue\">among</span> these are <span style=\"color:blue\">Life</span>, <span style=\"color:green\">Liberty</span>, and the <span style=\"color:red\">pursuit</span> of <span style=\"color:blue\">Happiness</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second implementation defined the number of topics to be 10.\n",
    "\n",
    "\n",
    "\n",
    "**American Speech Topics: 10 Topics:**\n",
    "\n",
    "| Topic | Most Probable Words |\n",
    "|-----------|----------|\n",
    "|      0 |    us, let, time, come, justice  |\n",
    "|      1     |    will, let, state, people, time      |\n",
    "|      2     |    nation, new, dream, negro, one      |\n",
    "|      3     |     will, us, nation, freedom, new     |\n",
    "|       4    |    freedom, will, nation, right, power      |\n",
    "|        5   |    freedom, day, nation, can, every      |\n",
    "|      6     |    us, freedom, people, today, right      |\n",
    "|     7      |     let, will, freedom, power, justice     |\n",
    "|     8      |      will, let, us, one, state    |\n",
    "|     9      |    will, let, nation, power, people      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Topic 0</span> \n",
    "<span style=\"color:blue\">Topic 1</span> \n",
    "<span style=\"color:green\">Topic 2</span> \n",
    "<span style=\"color:darkmagenta\">Topic 3</span> \n",
    "<span style=\"color:orange\">Topic 4</span> \n",
    "<span style=\"color:lightseagreen\">Topic 5</span> \n",
    "<span style=\"color:darkblue\">Topic 6</span> \n",
    "<span style=\"color:magenta\">Topic 7</span> \n",
    "<span style=\"color:cyan\">Topic 8</span> \n",
    "<span style=\"color:chartreuse\">Topic 9</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declaration of Independence: 10 Topics**\n",
    "\n",
    "When in the <span style=\"color:chartreuse\">Course</span> of <span style=\"color:lightseagreen\">human</span> <span style=\"color:red\">events</span>, it <span style=\"color:red\">becomes</span> <span style=\"color:green\">necessary</span> for <span style=\"color:blue\">one</span> <span style=\"color:orange\">people</span> to <span style=\"color:darkmagenta\">dissolve</span> the <span style=\"color:magenta\">political</span> <span style=\"color:cyan\">bands</span> \n",
    "which have <span style=\"color:darkblue\">connected</span> them with <span style=\"color:darkmagenta\">another</span>, and to <span style=\"color:darkmagenta\">assume</span>, <span style=\"color:blue\">among</span> the <span style=\"color:chartreuse\">Powers</span> of the <span style=\"color:chartreuse\">earth</span>, the <span style=\"color:chartreuse\">separate</span> and <span style=\"color:orange\">equal</span> \n",
    "<span style=\"color:lightseagreen\">station</span> to which the <span style=\"color:orange\">Laws</span> of <span style=\"color:lightseagreen\">Nature</span> and of <span style=\"color:lightseagreen\">Nature's</span> <span style=\"color:darkmagenta\">God</span> <span style=\"color:magenta\">entitle</span> them, a <span style=\"color:lightseagreen\">decent</span> <span style=\"color:green\">respect</span> to the <span style=\"color:darkmagenta\">opinions</span> of \n",
    "<span style=\"color:lightseagreen\">mankind</span> <span style=\"color:magenta\">requires</span> that they should \n",
    "<span style=\"color:lightseagreen\">declare</span> the <span style=\"color:orange\">causes</span> which <span style=\"color:lightseagreen\">impel</span> them to the <span style=\"color:chartreuse\">separation</span>.\n",
    "\n",
    "We <span style=\"color:magenta\">hold</span> these <span style=\"color:blue\">truths</span> to be <span style=\"color:chartreuse\">self-evident</span>, that all <span style=\"color:red\">men</span> are <span style=\"color:chartreuse\">created</span> <span style=\"color:orange\">equal</span>, that they are <span style=\"color:blue\">endowed</span> by their \n",
    "<span style=\"color:lightseagreen\">Creator</span> with <span style=\"color:orange\">certain</span> <span style=\"color:chartreuse\">unalienable</span> <span style=\"color:lightseagreen\">Rights</span>, that <span style=\"color:blue\">among</span> these are <span style=\"color:red\">Life</span>, <span style=\"color:darkblue\">Liberty</span>, and the <span style=\"color:cyan\">pursuit</span> of <span style=\"color:magenta\">Happiness</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gettysburg Address: 10 Topics**\n",
    "\n",
    "It is for <span style=\"color:lightseagreen\">us</span> the <span style=\"color:red\">living</span>, <span style=\"color:orange\">rather</span>, to be <span style=\"color:orange\">dedicated</span> here to the <span style=\"color:orange\">unfinished</span>\n",
    "<span style=\"color:magenta\">work</span> which they who <span style=\"color:cyan\">fought</span> here have <span style=\"color:orange\">thus</span> <span style=\"color:darkblue\">far</span> so <span style=\"color:lightseagreen\">nobly</span> <span style=\"color:lightseagreen\">advanced</span>.\n",
    "It is <span style=\"color:orange\">rather</span> for <span style=\"color:lightseagreen\">us</span> to be here <span style=\"color:orange\">dedicated</span> to the <span style=\"color:red\">great</span> <span style=\"color:lightseagreen\">task</span> <span style=\"color:chartreuse\">remaining</span>\n",
    "before <span style=\"color:lightseagreen\">us</span>. . .that from these <span style=\"color:red\">honored</span> <span style=\"color:darkblue\">dead</span> we <span style=\"color:magenta\">take</span> <span style=\"color:orange\">increased</span> <span style=\"color:green\">devotion</span>\n",
    "to that <span style=\"color:orange\">cause</span> for which they <span style=\"color:orange\">gave</span> the <span style=\"color:orange\">last</span> <span style=\"color:blue\">full</span> <span style=\"color:magenta\">measure</span> of <span style=\"color:green\">devotion</span>. . .\n",
    "that we here <span style=\"color:darkblue\">highly</span> <span style=\"color:darkblue\">resolve</span> that these <span style=\"color:darkblue\">dead</span> <span style=\"color:magenta\">shall</span> not have <span style=\"color:darkblue\">died</span> in <span style=\"color:orange\">vain</span>. . .\n",
    "that this <span style=\"color:lightseagreen\">nation</span>, under <span style=\"color:darkmagenta\">God</span>, <span style=\"color:magenta\">shall</span> have a <span style=\"color:darkblue\">new</span> <span style=\"color:lightseagreen\">birth</span> of <span style=\"color:cyan\">freedom</span>. . .\n",
    "and that <span style=\"color:darkblue\">government</span> of the <span style=\"color:orange\">people</span>. . .by the <span style=\"color:orange\">people</span>. . .for the <span style=\"color:orange\">people</span>. . .\n",
    "<span style=\"color:magenta\">shall</span> not <span style=\"color:lightseagreen\">perish</span> from this <span style=\"color:chartreuse\">earth</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary** \n",
    "\n",
    "For both the 3 topic and 10 topic LDA implmentations, the topic assignments make some sense, though to improve results, common words among all documents in the corpus should be removed or down-weighted and perhaps smoothing of the multinomial parameters should be performed as suggested in Section 5.4 of the orginial LDA paper (Blei et. al., 1006).\n",
    "\n",
    "For 3 topics, words like happiness, rights, life and endowed are assigned the highest probability of being in the same topic and words like creator, God, power and liberty are also assigned to the same topic.  For 10 topics, however, it appears that there may be too many topics.  Creator and God are no long assigned to the same topic and neither is dead and perish.  However, the addition of more topics does allow for words like Nature, Creator and Mankind to be grouped together.  Likely somewhere between 3 and 10 topics would be optimal for this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('Gettysburg.txt', 'r')\n",
    "Gettysburg = file.read()\n",
    "file2 = open(\"Dec_of_Independence.txt\", 'r')\n",
    "Declaration = file2.read()\n",
    "file3 = open(\"MLK_Dream.txt\", 'r')\n",
    "MLK = file3.read()\n",
    "file4 = open(\"JFK_Inauguration.txt\", 'r')\n",
    "JFK = file4.read()\n",
    "\n",
    "speechDocs = collections.OrderedDict()\n",
    "speechDocs[\"s1\"] = Gettysburg\n",
    "speechDocs[\"s2\"] = Declaration\n",
    "speechDocs[\"s3\"] = MLK\n",
    "speechDocs[\"s4\"] = JFK\n",
    "\n",
    "speechCorp = make_word_matrix(speechDocs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "ResultsSpeech = LDA(3, speechCorp, 0.1)\n",
    "ResultsSpeech2 = LDA(10, speechCorp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annayanchenko/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: FutureWarning: sort is deprecated, use sort_values(inplace=True) for for INPLACE sorting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[               0\n",
       " us      0.017606\n",
       " let     0.011493\n",
       " time    0.011116\n",
       " come    0.010975\n",
       " justic  0.010577,               1\n",
       " will   0.019539\n",
       " let    0.015517\n",
       " state  0.013654\n",
       " peopl  0.013193\n",
       " time   0.011894,                2\n",
       " nation  0.019027\n",
       " new     0.014987\n",
       " dream   0.014013\n",
       " negro   0.012757\n",
       " one     0.012178,                 3\n",
       " will     0.023211\n",
       " us       0.021747\n",
       " nation   0.014964\n",
       " freedom  0.014338\n",
       " new      0.014234,                 4\n",
       " freedom  0.019594\n",
       " will     0.019362\n",
       " nation   0.018079\n",
       " right    0.013548\n",
       " power    0.012865,                 5\n",
       " freedom  0.015909\n",
       " day      0.012938\n",
       " nation   0.012780\n",
       " can      0.011266\n",
       " everi    0.010557,                 6\n",
       " us       0.027105\n",
       " freedom  0.018973\n",
       " peopl    0.014059\n",
       " today    0.012804\n",
       " right    0.011288,                 7\n",
       " let      0.024760\n",
       " will     0.023571\n",
       " freedom  0.017365\n",
       " power    0.011775\n",
       " justic   0.010406,               8\n",
       " will   0.025153\n",
       " let    0.024609\n",
       " us     0.020811\n",
       " one    0.012393\n",
       " state  0.011591,                9\n",
       " will    0.018517\n",
       " let     0.017733\n",
       " nation  0.013431\n",
       " power   0.012510\n",
       " peopl   0.011205]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommon(ResultsSpeech2[0][3], speechCorp[1], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to Assign Words to the Topic in which they Occur with the Highest Probability**\n",
    "\n",
    "**colorCode(beta, wordList, inputWords):**\n",
    "\n",
    "**Input:**\n",
    "- *beta*: the output for beta from the LDA function, a k x V matrix\n",
    "- *wordList*:the list of the unique words in the vocabulary in the corpus that LDA was run on to find beta, the output of the make_word_matrix()[1]\n",
    "- *inputWords*: a sample of text from the corpus to assign each word to a topic\n",
    "\n",
    "**Output: pd.DataFrame(topicWords)**\n",
    "- *topicWords*: a pandas Data Frame where each row is a word from inputWords and the column is the topic under which that word occurs with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def colorCode(beta, wordList, inputWords):\n",
    "    \n",
    "    #Split text, convert to lowercase and remove punctuation and stop words\n",
    "    stopWords = get_stop_words('english')\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "    text = inputWords.translate(table)\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if word not in stopWords]\n",
    "    p_stemmer = PorterStemmer()\n",
    "    words = [p_stemmer.stem(word) for word in words]\n",
    "    betaDF = pd.DataFrame(beta)\n",
    "    betaDF.columns = wordList\n",
    "    \n",
    "    \n",
    "    k = len(words)\n",
    "    topicWords = np.zeros((1,k))\n",
    "    topicWords = pd.DataFrame(topicWords,  columns=words)\n",
    "\n",
    "    for i in range(0, k):\n",
    "        document = np.array(betaDF.loc[:,words[i]])\n",
    "        topicWords.iloc[:,i] = np.argmax(document)\n",
    "    return(topicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GSample = \"\"\"It is for us the living, rather, to be dedicated here to the unfinished\n",
    "work which they who fought here have thus far so nobly advanced.\n",
    "It is rather for us to be here dedicated to the great task remaining\n",
    "before us. . .that from these honored dead we take increased devotion\n",
    "to that cause for which they gave the last full measure of devotion. . .\n",
    "that we here highly resolve that these dead shall not have died in vain. . .\n",
    "that this nation, under God, shall have a new birth of freedom. . .\n",
    "and that government of the people. . .by the people. . .for the people. . .\n",
    "shall not perish from this earth.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = colorCode(ResultsSpeech2[0][3], speechCorp[1], GSample).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ii. Movie Data*\n",
    "\n",
    "In addition to the topic creation for the historical documents, the LDA algorithm was used to cluster movies based on user ratings. This example used the MovieLens data set from the GroupLens website (http://grouplens.org/datasets/movielens/).  This data contains user ratings for 2,714 unique movies, and we use the data to determine clusters of movies based on which movies the users prefer. A movie is preferred if a user rated it four or five (out of five). The data was then reduced to only the information about users who had at least 50 preferred movies. In this example, the users can be thought of as the documents and their preferred movies are the words. The corpus is the collection of users. LDA will create topics for these movies that should cluster movies together based on which user preferred which movies. The LDA algorithm was used to create five clusters of movies. If a movie appeared in more than one cluster, it was only assigned to the cluster for which it had the highest probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading in the data\n",
    "import pandas as pd\n",
    "Ratings = pd.read_csv(\"RatingsDataClean.csv\")\n",
    "movieData = pd.read_csv(\"MovieReferenceTable.csv\", encoding='cp1252')\n",
    "\n",
    "\n",
    "#making a dictionary for the training and testing data\n",
    "Users = list(set(Ratings['userID']))\n",
    "RatingsDict = {}\n",
    "for i in range(0, len(Users)):\n",
    "    movies = list(map(str, list(pd.DataFrame(Ratings[Ratings['userID'] == Users[i]])['movieID'])))\n",
    "    RatingsDict[i] = ' '.join(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#running LDA analysis to get parameters for movie training data\n",
    "import random\n",
    "\n",
    "random.seed(77)\n",
    "movieMatrix = make_word_matrix(RatingsDict, 1)\n",
    "movieOutput = LDA(5, movieMatrix, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annayanchenko/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: FutureWarning: sort is deprecated, use sort_values(inplace=True) for for INPLACE sorting\n"
     ]
    }
   ],
   "source": [
    "movieTopics = mostCommon(movieOutput[0][3], movieMatrix[1], 15)\n",
    "movieList = []\n",
    "for i in range(0,5):\n",
    "    movieList.append(list(np.array(movieTopics[i].index, dtype = int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "betaDF = movieOutput[0][3]\n",
    "\n",
    "k = len(movieMatrix[1])\n",
    "topicWords = np.zeros((1,k))\n",
    "topicWords = pd.DataFrame(topicWords,  columns=movieMatrix[1])\n",
    "\n",
    "for i in range(0, k):\n",
    "    document = np.array(betaDF[:,i])\n",
    "    topicWords.iloc[:,i] = np.argmax(document)\n",
    "\n",
    "topicWords = topicWords.T\n",
    "\n",
    "topicWords['movieID'] = topicWords.index.astype(int)\n",
    "\n",
    "movieData = topicWords.merge(movieData, on = \"movieID\", how = \"left\")\n",
    "movieData.columns = ['cluster', 'movieID', 'UserID', \"Name\", \"Genre\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1**\n",
    "\n",
    "The first cluster appears to be mostly composed of children's and family movies. One exception is Freeway, an R-rated  comedy, crime and thriller movie. In addition, Golden Eye is a James Bond movie that does not seem to very similar to the other movies in this cluster. Freeway and Golden Eye are probably movies enjoyed by similar viewers. So it is possible that this cluster is the reuslt of parents who watch a lot of children's movies but also enjoy action movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>movieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1007</td>\n",
       "      <td>994</td>\n",
       "      <td>Apple Dumpling Gang, The (1975)</td>\n",
       "      <td>Children's|Comedy|Western</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1018</td>\n",
       "      <td>1005</td>\n",
       "      <td>That Darn Cat! (1965)</td>\n",
       "      <td>Children's|Comedy|Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>1041</td>\n",
       "      <td>1028</td>\n",
       "      <td>Secrets &amp; Lies (1996)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>1049</td>\n",
       "      <td>1035</td>\n",
       "      <td>Ghost and the Darkness, The (1996)</td>\n",
       "      <td>Action|Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>1066</td>\n",
       "      <td>1052</td>\n",
       "      <td>Shall We Dance? (1937)</td>\n",
       "      <td>Comedy|Musical|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>1067</td>\n",
       "      <td>1053</td>\n",
       "      <td>Damsel in Distress, A (1937)</td>\n",
       "      <td>Comedy|Musical|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>1073</td>\n",
       "      <td>1058</td>\n",
       "      <td>Willy Wonka and the Chocolate Factory (1971)</td>\n",
       "      <td>Adventure|Children's|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>1080</td>\n",
       "      <td>1064</td>\n",
       "      <td>Monty Python's Life of Brian (1979)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>1089</td>\n",
       "      <td>1073</td>\n",
       "      <td>Reservoir Dogs (1992)</td>\n",
       "      <td>Crime|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster  movieID  UserID                                          Name  \\\n",
       "0         0        1       0                              Toy Story (1995)   \n",
       "6         0     1007     994               Apple Dumpling Gang, The (1975)   \n",
       "18        0     1018    1005                         That Darn Cat! (1965)   \n",
       "39        0     1041    1028                         Secrets & Lies (1996)   \n",
       "45        0     1049    1035            Ghost and the Darkness, The (1996)   \n",
       "59        0     1066    1052                        Shall We Dance? (1937)   \n",
       "60        0     1067    1053                  Damsel in Distress, A (1937)   \n",
       "64        0     1073    1058  Willy Wonka and the Chocolate Factory (1971)   \n",
       "69        0     1080    1064           Monty Python's Life of Brian (1979)   \n",
       "77        0     1089    1073                         Reservoir Dogs (1992)   \n",
       "\n",
       "                                  Genre  \n",
       "0           Animation|Children's|Comedy  \n",
       "6             Children's|Comedy|Western  \n",
       "18            Children's|Comedy|Mystery  \n",
       "39                                Drama  \n",
       "45                     Action|Adventure  \n",
       "59               Comedy|Musical|Romance  \n",
       "60               Comedy|Musical|Romance  \n",
       "64  Adventure|Children's|Comedy|Fantasy  \n",
       "69                               Comedy  \n",
       "77                       Crime|Thriller  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cluster 1\n",
    "movieData[movieData['cluster'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 2**\n",
    "\n",
    "This cluster has fewer children's movies than Cluster 1. Of the non-children's movies, the remaining movies are mostly drama or thriller.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>movieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>City Hall (1996)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>995</td>\n",
       "      <td>Davy Crockett, King of the Wild Frontier (1955)</td>\n",
       "      <td>Western</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1009</td>\n",
       "      <td>996</td>\n",
       "      <td>Escape to Witch Mountain (1975)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>99</td>\n",
       "      <td>Bottle Rocket (1996)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1011</td>\n",
       "      <td>998</td>\n",
       "      <td>Herbie Rides Again (1974)</td>\n",
       "      <td>Adventure|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1027</td>\n",
       "      <td>1014</td>\n",
       "      <td>Robin Hood:  Prince of Thieves (1991): Drama: ...</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1031</td>\n",
       "      <td>1018</td>\n",
       "      <td>Bedknobs and Broomsticks (1971)</td>\n",
       "      <td>Adventure|Children's|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>1034</td>\n",
       "      <td>1021</td>\n",
       "      <td>Freeway (1996)</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1039</td>\n",
       "      <td>1026</td>\n",
       "      <td>Synthetic Pleasures (1995)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster  movieID  UserID  \\\n",
       "1         1       10       9   \n",
       "2         1      100      98   \n",
       "7         1     1008     995   \n",
       "8         1     1009     996   \n",
       "9         1      101      99   \n",
       "11        1     1011     998   \n",
       "26        1     1027    1014   \n",
       "30        1     1031    1018   \n",
       "33        1     1034    1021   \n",
       "37        1     1039    1026   \n",
       "\n",
       "                                                 Name  \\\n",
       "1                                    GoldenEye (1995)   \n",
       "2                                    City Hall (1996)   \n",
       "7     Davy Crockett, King of the Wild Frontier (1955)   \n",
       "8                     Escape to Witch Mountain (1975)   \n",
       "9                                Bottle Rocket (1996)   \n",
       "11                          Herbie Rides Again (1974)   \n",
       "26  Robin Hood:  Prince of Thieves (1991): Drama: ...   \n",
       "30                    Bedknobs and Broomsticks (1971)   \n",
       "33                                     Freeway (1996)   \n",
       "37                         Synthetic Pleasures (1995)   \n",
       "\n",
       "                           Genre  \n",
       "1      Action|Adventure|Thriller  \n",
       "2                 Drama|Thriller  \n",
       "7                        Western  \n",
       "8   Adventure|Children's|Fantasy  \n",
       "9                         Comedy  \n",
       "11   Adventure|Children's|Comedy  \n",
       "26                         Drama  \n",
       "30  Adventure|Children's|Musical  \n",
       "33                         Crime  \n",
       "37                   Documentary  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cluster 2\n",
    "movieData[movieData['cluster'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 3**\n",
    "\n",
    "This cluster also consistents of many children's movies. The movies that are not children's movies are the comedies Ed's Next Move, Bottle Rocket and That Thing You Do! This could be another instance of parents rating movies,  but instead of action movies they enjoy watching comedies when their children are not around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>movieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1002</td>\n",
       "      <td>989</td>\n",
       "      <td>Ed's Next Move (1996)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1005</td>\n",
       "      <td>992</td>\n",
       "      <td>D3:  The Mighty Ducks (1996): Children's|Comed...</td>\n",
       "      <td>Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>1014</td>\n",
       "      <td>1001</td>\n",
       "      <td>Pollyanna (1960)</td>\n",
       "      <td>Children's|Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1016</td>\n",
       "      <td>1003</td>\n",
       "      <td>Shaggy Dog, The (1959)</td>\n",
       "      <td>Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1017</td>\n",
       "      <td>1004</td>\n",
       "      <td>Swiss Family Robinson (1960)</td>\n",
       "      <td>Adventure|Children's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1020</td>\n",
       "      <td>1007</td>\n",
       "      <td>Cool Runnings (1993)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>1011</td>\n",
       "      <td>Three Caballeros, The (1945)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>1029</td>\n",
       "      <td>1016</td>\n",
       "      <td>Dumbo (1941)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>1030</td>\n",
       "      <td>1017</td>\n",
       "      <td>Pete's Dragon (1977)</td>\n",
       "      <td>Adventure|Animation|Children's|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>1033</td>\n",
       "      <td>1020</td>\n",
       "      <td>Fox and the Hound, The (1981)</td>\n",
       "      <td>Animation|Children's</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster  movieID  UserID  \\\n",
       "3         2     1002     989   \n",
       "5         2     1005     992   \n",
       "14        2     1014    1001   \n",
       "16        2     1016    1003   \n",
       "17        2     1017    1004   \n",
       "20        2     1020    1007   \n",
       "24        2     1024    1011   \n",
       "28        2     1029    1016   \n",
       "29        2     1030    1017   \n",
       "32        2     1033    1020   \n",
       "\n",
       "                                                 Name  \\\n",
       "3                               Ed's Next Move (1996)   \n",
       "5   D3:  The Mighty Ducks (1996): Children's|Comed...   \n",
       "14                                   Pollyanna (1960)   \n",
       "16                             Shaggy Dog, The (1959)   \n",
       "17                       Swiss Family Robinson (1960)   \n",
       "20                               Cool Runnings (1993)   \n",
       "24                       Three Caballeros, The (1945)   \n",
       "28                                       Dumbo (1941)   \n",
       "29                               Pete's Dragon (1977)   \n",
       "32                      Fox and the Hound, The (1981)   \n",
       "\n",
       "                                     Genre  \n",
       "3                                   Comedy  \n",
       "5                        Children's|Comedy  \n",
       "14                 Children's|Comedy|Drama  \n",
       "16                       Children's|Comedy  \n",
       "17                    Adventure|Children's  \n",
       "20                                  Comedy  \n",
       "24            Animation|Children's|Musical  \n",
       "28            Animation|Children's|Musical  \n",
       "29  Adventure|Animation|Children's|Musical  \n",
       "32                    Animation|Children's  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cluster 3\n",
    "movieData[movieData['cluster'] == 2].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 4**\n",
    "\n",
    "Cluster 4 had several musical children's movies and among the non-children's movies, contains many comedies and drama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>movieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1003</td>\n",
       "      <td>990</td>\n",
       "      <td>Extreme Measures (1996)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1012</td>\n",
       "      <td>999</td>\n",
       "      <td>Old Yeller (1957)</td>\n",
       "      <td>Children's|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>1022</td>\n",
       "      <td>1009</td>\n",
       "      <td>Cinderella (1950)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>1032</td>\n",
       "      <td>1019</td>\n",
       "      <td>Alice in Wonderland (1951)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>1035</td>\n",
       "      <td>1022</td>\n",
       "      <td>Sound of Music, The (1965)</td>\n",
       "      <td>Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>102</td>\n",
       "      <td>Happy Gilmore (1996)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>1043</td>\n",
       "      <td>1030</td>\n",
       "      <td>To Gillian on Her 37th Birthday (1996)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>1046</td>\n",
       "      <td>1033</td>\n",
       "      <td>Beautiful Thing (1996)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>1050</td>\n",
       "      <td>1036</td>\n",
       "      <td>Looking for Richard (1996)</td>\n",
       "      <td>Documentary|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>1054</td>\n",
       "      <td>1040</td>\n",
       "      <td>Get on the Bus (1996)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster  movieID  UserID                                    Name  \\\n",
       "4         3     1003     990                 Extreme Measures (1996)   \n",
       "12        3     1012     999                       Old Yeller (1957)   \n",
       "22        3     1022    1009                       Cinderella (1950)   \n",
       "31        3     1032    1019              Alice in Wonderland (1951)   \n",
       "34        3     1035    1022              Sound of Music, The (1965)   \n",
       "38        3      104     102                    Happy Gilmore (1996)   \n",
       "41        3     1043    1030  To Gillian on Her 37th Birthday (1996)   \n",
       "43        3     1046    1033                  Beautiful Thing (1996)   \n",
       "47        3     1050    1036              Looking for Richard (1996)   \n",
       "50        3     1054    1040                   Get on the Bus (1996)   \n",
       "\n",
       "                           Genre  \n",
       "4                 Drama|Thriller  \n",
       "12              Children's|Drama  \n",
       "22  Animation|Children's|Musical  \n",
       "31  Animation|Children's|Musical  \n",
       "34                       Musical  \n",
       "38                        Comedy  \n",
       "41                 Drama|Romance  \n",
       "43                 Drama|Romance  \n",
       "47             Documentary|Drama  \n",
       "50                         Drama  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cluster 4\n",
    "movieData[movieData['cluster'] == 3].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 5**\n",
    "\n",
    "Cluster 5 also has a lot of children's movies. The only movies that are not children's movies belong to the action/thriller genre. Again, these users may watch children's movies with their family, but prefer action movies when it is not family time. The children's movies are older, so it is possible that the family movie preferences have transitioned as the children grow up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>movieID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>1010</td>\n",
       "      <td>997</td>\n",
       "      <td>Love Bug, The (1969)</td>\n",
       "      <td>Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1013</td>\n",
       "      <td>1000</td>\n",
       "      <td>Parent Trap, The (1961)</td>\n",
       "      <td>Children's|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>1015</td>\n",
       "      <td>1002</td>\n",
       "      <td>Homeward Bound:  The Incredible Journey (1993)...</td>\n",
       "      <td>Adventure|Children's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1019</td>\n",
       "      <td>1006</td>\n",
       "      <td>20,000 Leagues Under the Sea (1954)</td>\n",
       "      <td>Adventure|Children's|Fantasy|Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1021</td>\n",
       "      <td>1008</td>\n",
       "      <td>Angels in the Outfield (1994)</td>\n",
       "      <td>Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>1023</td>\n",
       "      <td>1010</td>\n",
       "      <td>Winnie the Pooh and the Blustery Day (1968)</td>\n",
       "      <td>Animation|Children's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1025</td>\n",
       "      <td>1012</td>\n",
       "      <td>Sword in the Stone, The (1963)</td>\n",
       "      <td>Animation|Children's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1028</td>\n",
       "      <td>1015</td>\n",
       "      <td>Mary Poppins (1964)</td>\n",
       "      <td>Children's|Comedy|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>1037</td>\n",
       "      <td>1024</td>\n",
       "      <td>Lawnmower Man, The (1992)</td>\n",
       "      <td>Action|Sci-Fi|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>1047</td>\n",
       "      <td>1034</td>\n",
       "      <td>Long Kiss Goodnight, The (1996)</td>\n",
       "      <td>Action|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster  movieID  UserID  \\\n",
       "10        4     1010     997   \n",
       "13        4     1013    1000   \n",
       "15        4     1015    1002   \n",
       "19        4     1019    1006   \n",
       "21        4     1021    1008   \n",
       "23        4     1023    1010   \n",
       "25        4     1025    1012   \n",
       "27        4     1028    1015   \n",
       "36        4     1037    1024   \n",
       "44        4     1047    1034   \n",
       "\n",
       "                                                 Name  \\\n",
       "10                               Love Bug, The (1969)   \n",
       "13                            Parent Trap, The (1961)   \n",
       "15  Homeward Bound:  The Incredible Journey (1993)...   \n",
       "19                20,000 Leagues Under the Sea (1954)   \n",
       "21                      Angels in the Outfield (1994)   \n",
       "23        Winnie the Pooh and the Blustery Day (1968)   \n",
       "25                     Sword in the Stone, The (1963)   \n",
       "27                                Mary Poppins (1964)   \n",
       "36                          Lawnmower Man, The (1992)   \n",
       "44                    Long Kiss Goodnight, The (1996)   \n",
       "\n",
       "                                  Genre  \n",
       "10                    Children's|Comedy  \n",
       "13                     Children's|Drama  \n",
       "15                 Adventure|Children's  \n",
       "19  Adventure|Children's|Fantasy|Sci-Fi  \n",
       "21                    Children's|Comedy  \n",
       "23                 Animation|Children's  \n",
       "25                 Animation|Children's  \n",
       "27            Children's|Comedy|Musical  \n",
       "36               Action|Sci-Fi|Thriller  \n",
       "44                      Action|Thriller  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cluster 5\n",
    "movieData[movieData['cluster'] == 4].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.\tFurther Research\n",
    "\n",
    "One area in which we would like to expand our research and improve our project is in the approximation of the posterior. In the implementation for this project, variational inference was utilized to estimate the intractable posterior distribution. However, there are other methods that can be used to estimate the posterior. These include Gibbs sampling and Metropolis-Hastings. It is possible that these approximations might increase the speed of the implementation. \n",
    "Another area to expand on is the determination of the number of topics. The R implementation of LDA provides different measures to determine the number of topics, and these are something that we can look into adding to our code.\n",
    "\n",
    "In order to improve the topic definitions, we want to investigate weighting terms by the number of times that they appear in the documents. Many of the results  in this report contain topics that have the same words in each of them. It is sensical that the same word may appear in documents on similar topics, such as American historical documents, but if a word appears many times in each document, that word will not be very helpful in defining topics. Another way to improve topic definition would be to implement smoothing into the algorithm. If a word only occurs once in a corpus, it will have a very small probability. However, this word could be important in defining the topic for a particular document. Smoothing would ensure that every word would be assigned a positive probability. \n",
    "\n",
    "In addition, we want to continue to use the algorithms to situations beyond text corpora. The movie data example in this paper is one example of how LDA can be utilized beyond text data. The algorithm can be used for any situation that has the same structure as text corpora. The possibilities for LDA applications are endless, and we hope to explore more of these situations in the future. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. References\n",
    "\n",
    "1.\tColorado Reed, Latent Dirichlet Allocation: Towards a Deeper Understanding, January 2012, o\n",
    "bphio.us/pdfs/lda_tutorial.pdf. \n",
    "2.\tDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan, Latent Dirichlet Allocation, Journal of \n",
    "Machine Learning Research 3, 2003, pg. 993-1022.\n",
    "3. Declaration of Independence: http://www.gutenberg.org/files/16780/16780-h/16780-h.html\n",
    "4. F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\n",
    "5. Gettysburg Address: http://www.gutenberg.org/cache/epub/4/pg4.txt.\n",
    "6. Internet Movie Database, www.imbd.com.\n",
    "7. JFK Speech: http://www.americanrhetoric.com/speeches/jfkinaugural.htm\n",
    "8. Max Sklar, Fast MLE Computation for the Dirichlet Multinomial, May 2014.\n",
    "9. MLK Speech:  http://www.americanrhetoric.com/speeches/mlkihaveadream.htm\n",
    "10. Thomas P. Minka, Estimating a Dirichlet Distribution, www.msr-waypoint.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
